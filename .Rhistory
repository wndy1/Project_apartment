lh_dist = (theta_1^nx)*((1-theta_1)^(5-nx))
lh_dist
# x에 대한 marginal dist
marg_dist = sum(lh_dist*prob_theta)
# posterior dist
pos_dist = (lh_dist*prob_theta)/marg_dist
pos_dist
# theta = 0.5일때 posterior dist 확률
pos_dist[5]
### 3장 4번
# x의 개수
# (x1,x2,x3,x4,x5) = (1,1,0,0,1)
nx = 3
# 사전분포는 이산균일분포를 따른다
theta_1 = seq(0.1,0.9,by=0.1)
prior_theta = 1/9
# 가능도 함수
lh_dist = (theta_1^nx)*((1-theta_1)^(5-nx))
lh_dist
# x에 대한 marginal dist
marg_dist = sum(lh_dist*prob_theta)
# posterior dist
pos_dist = (lh_dist*prob_theta)/marg_dist
rm(list=ls())
### 3장 4번
# x의 개수
# (x1,x2,x3,x4,x5) = (1,1,0,0,1)
nx = 3
# 사전분포는 이산균일분포를 따른다
theta_1 = seq(0.1,0.9,by=0.1)
prior_theta = 1/9
# 가능도 함수
lh_dist = (theta_1^nx)*((1-theta_1)^(5-nx))
lh_dist
# x에 대한 marginal dist
marg_dist = sum(lh_dist*prior_theta)
# posterior dist
pos_dist = (lh_dist*prior_theta)/marg_dist
pos_dist
# theta = 0.5일때 posterior dist 확률
pos_dist[5]
windows()
plot(x = theta, y = pos_dist)
plot(x = theta_1, y = pos_dist)
windows()
plot(x = theta_1,type='h', y = pos_dist)
### 3장 5번
# (1)
lh_dist[5]
# 0.03125
# (2)
marg_dist
### 5장 2번
## (1)
a = 2; b = 8; n = 10; x = 2
theta = seq(0,1,length=100)
prior.theta = dbeta(theta,a,b)
post.theta = dbeta(theta,a+x,n-x+b) # conjugate prior에 따른 사후분포 또한 베타분포를 따른다
windows()
plot(x = theta,y=post.theta,col=2,type='l')
lines(x = theta,y= prior.theta)
legend("topright",legend=c(paste("beta(",a+x,",",n-x+b,") posterior"),
paste("beta(",a,",",b,") prior")),lty=c(1,1), col=c(2,"black"))
## (3)
# theta의 사후 표본을 이용하는 HPD구간 함수
HPDsample = function(theta,level = 0.95){
N=length(theta) #표본의 수
theta.sort = sort(theta) # 오름차순 정렬
M = ceiling(N*level) # 신뢰구간을 따르는 표본의 수
nCI = N-M # 가능한 신뢰구간의 수
CI.width = rep(0,nCI) # 가능한 신뢰구간의 수만큼 벡터 생성
for(i in 1:nCI)CI.width[i] = theta.sort[i+M]-theta.sort[i] # 신뢰구간의 길이 계산
index = which.min(CI.width) # 구간 중 가장 짧은 길이의 인덱스 값
HPD = c(theta.sort[index],theta.sort[index+M]) # 해당 길이의 theta값
return(HPD)
}
# theta의 사후 표본 생성
set.seed(1234) # 씨드 설정
pos_theta = rbeta(100,a+x,n-x+b)
# 95%수준의 최대사후구간
HPDsample(theta = pos_theta,level = 0.95)
pri_theta = rbeta(1000,a,b)
# theta의 사후표본
pos_theta2 = rbeta(1000,a+x,n-x+b)
# 사전표본 오즈비
pri_eta = log(pri_theta/(1-pri_theta))
# 사후표본 오즈비
pos_eta = log(pos_theta2/(1-pos_theta2))
# 최대사후구간
HPD = HPDsample(theta = pos_eta,level = 0.95)
windows()
plot(density(pos_eta),type='l',lwd = 2,lty=2,xlab="eta",ylab = "density",
main = "사전표본 오즈비와 사후표본 오즈비 비교")
lines(density(pri_eta),lwd=1, lty=1)
abline(v = HPD,lwd = 2, lty =3 ,col=2)
legend("topright",legend=c("사후밀도함수","사전밀도함수","최대사후구간"),lty = c(2,1,3),
col = c("black","black","red"))
### 5장 4번
n = 12; x = 3
# 12번의 독립적인 베루느이 시행 중 세번의 성공이 관측이라 가정
# 사전분포를 1 즉 beta(1,1)로 둔다.
a = 1; b = 1
# posterior dist
theta = seq(0,1,length=50)
post.dist = dbeta(theta,a+x,n-x+b)
# X_13 = 1 의 예측 확률은 결국 (a+x)/(a+b+n)이므로 사후분포의 평균 값이다.
X_13.prob = (a+x)/(a+b+n)
X_13.prob
### 5장 6번
# 그룹 1
n1 = 20; x1 = 10
# 그룹 2
n2 = 20; x2 = 14
# 사전분포는 beta(1,1)
a = 1; b = 1
# 그룹1의 사후분포는 beta(1+10,20-10+10) -> beta(11,20)이고
# 그룹2의 사후분포는 beta(1+14,20-14+1) -> beta(15,7) 이다.
# 각그룹별 사후표본 생성
theta_1 = rbeta(1000,11,20)
theta_2 = rbeta(1000,15,7)
# 로그오즈비
xi = log((theta_1/(1-theta_1)) / (theta_2/(1-theta_2)))
# 몬테칼로 방법(결국 사후표본으로 추정) 로그오즈비 추정치와 표준오차 게산
mean(xi); sd(xi)
# 로그오즈비 사후밀도함수와 95% HPD 표시
HPD = HPDsample(theta = xi,level = 0.95)
windows()
plot(density(xi),type = 'l',lwd = 3,ylab = 'density',xlab = '로즈오즈비',
main = '로그오즈비 사후밀도함수와 HPD')
abline(v = HPD,lwd=2,lty=2,col=2)
legend("topright",legend=c("사후밀도함수","95% 최대사후구간"),lwd=c(3,2),lty = c(1,2),col = c("black",2))
rm(list=ls())
y1 = rnorm(100,4,1)
y2 = rnorm(50,6,2)
windows()
par(mfrow=c(1,2))
hist(y1)
hist(y2)
windows()
par(mfrow=c(1,2))
hist(y1)
hist(y2)
windows()
par(mfrow=c(2,1))
hist(y1)
hist(y2)
summary(y1)
summary(y2)
windows()
par(mfrow=c(2,1))
hist(y1,xlim = c(0,11))
hist(y2,xlim = c(0,11))
abline(v=mean(y1),col=2)
summary(y1)
abline(v=mean(y2),col=2)
windows()
par(mfrow=c(2,1))
hist(y1,xlim = c(0,11))
abline(v=mean(y1),col=2)
hist(y2,xlim = c(0,11))
abline(v=mean(y2),col=2)
windwos()
hist(y1,xlim=c(0,11),col=rgb(1,0,0,0.5),freq = F)
hist(y1,xlim=c(0,11),col=rgb(0,0,0,0.5),freq = F ,add=T)
windows()
hist(y1,xlim=c(0,11),col=rgb(1,0,0,0.5),freq = F)
hist(y1,xlim=c(0,11),col=rgb(0,0,0,0.5),freq = F ,add=T)
windows()
hist(y1,xlim=c(0,11),col=rgb(1,0,0,0.5),freq = F)
hist(y2,xlim=c(0,11),col=rgb(0,0,0,0.5),freq = F ,add=T)
windows()
hist(y1,xlim=c(min(y1)-1,max(y1)+1),col=rgb(1,0,0,0.5),freq = F)
hist(y2,xlim=c(min(y1)-1,max(y1)+1),col=rgb(0,0,0,0.5),freq = F ,add=T)
# density plot
d1 = density(y1)
d2 = density(y2)
windows()
plot(d1,xlim=c(0,11))
lines(d2,xlim=c(0,11),col=2)
windows()
plot(d1,xlim=c(0,11))
lines(d2,xlim=c(0,11),col=2)
windows()
qqplot(y1,y2)
windows()
qqplot(y1,y2)
abline(a=0,b=1,col=2,lty=2)
tmp1 = rnorm(100,4,1)
tmp2 = rnorm(50,6,1)
windows()
qqplot(tmp1,tmp2)
abline(a=0,b=1,col=2,lty=2)
tmp1 = rnorm(100,4,1)
tmp2 = rnorm(50,6,3)
windows()
qqplot(tmp1,tmp2)
abline(a=0,b=1,col=2,lty=2)
windows()
qqplot(tmp1,tmp2)
abline(a=0,b=1,col=2,lty=2)
# 표준편차만 다를 경우 기울기는 다르다는 것을 확인할 수 있다.
graphics.off()
mean1 = mean(y1)
mean2 = mean(y2)
var1 = var(y1)
var2 = var(y2)
sd1 = sd(y1)
sd2=sd(y2)
n1=length(y1)
n2=length(y2)
mean1;mean2
var1;var2
# 정규성 검정
shapiro.test(y1)
shapiro.test(y2)
library(moments)
library(nortest)
lillie.test(y1)
lillie.test(y2)
#######
skewness(y1)
skewness(y2)
agostino.test(y1)
agostino.test(y2)
kurtosis(y1)
kurtosis(y2)
anscombe.test(y1)
anscombe.test(y2)
agostino.test(y1)
agostino.test(y2)
anscombe.test(y1)
anscombe.test(y2)
#######
skewness(y1)
skewness(y2)
agostino.test(y1)
agostino.test(y2)
kurtosis(y1)
kurtosis(y2)
anscombe.test(y1)
anscombe.test(y2)
lillie.test(y1)
lillie.test(y2)
# 틍분산 검정
var.test(y1,y2)
# 두 분산이 같다는 가정 하에 평균 검정(t 검정)
var_pooled = ((n1-1)*var1 + (n2-1)*var2))/(n1+n2-2)
# 두 분산이 같다는 가정 하에 평균 검정(t 검정)
var_pooled = ((n1-1)*var1 + (n2-1)*var2)/(n1+n2-2)
t1 = (mean1-mean2)/sqrt(var_pooled)
p_val = 2*pt(abs(t1),df=n1+n2-2)
p-val
p_val
p_val = 2*(1-pt(abs(t1),df=n1+n2-2))
p_val
t.test(y1,y2,var.equal = T)
t1
# 두 분산이 다르다는 가정 하 평균 검정
t2 = (mean1-mean2)/sqrt((var/n1 + var/n2))
# 두 분산이 다르다는 가정 하 평균 검정
t2 = (mean1-mean2)/sqrt((var1/n1 + var2/n2))
# ~ 자유도
w1 = var1/n1
w2 = var2/n2
df = (w1+w2)^2/(w1^2/(n1-1)+w2^2/(n2-1))
t.test(y1,y2,var.equal = F)
t2
df.sw = (w1+w2)^2/(w1^2/(n1-1)+w2^2/(n2-1))
pval2  = 2*(1-pt(abs(t2),df=df.sw))
pval2
t.test(y1,y2,var.equal = F)
# 교과서 자유도
df.1 = min(n1-1,n2-1)
pval3  = 2*(1-pt(abs(t2),df=df.1))
pval3
# 두모집단이 정규분포가 아니더라도 표본이 충분하다면
# 중심극한정리를 이용하여 검정을 수행할 수 있다.
pval4 = 2*(1-pnorm(abs(t2)))
pval4
# 비모수 검정
# 각 모집단이 정규분포를 따른다고 보기 힘들고 표본의 크기가 충분치 않을 경우
# 비모수 검정을 수행할 수 있다.
wilcox.test(y1,y2)
# 두분포가 다른디 검정
ks.test(y1,y2)
tmp1 = rnorm(100,4,1)
tmp2 = rnorm(50,6,1)
ks.test(tmp1,tmp2)
tmp1 = rnorm(100,6,1)
tmp2 = rnorm(50,6,4)
ks.test(tmp1,tmp2)
data(iris)
head(iris)
species = iris$Species
swidth = iris$Sepal.Width
# summary
summary(swidth)
windows()
hist(swidth)
library(installr)
updateR()
version
rm(list=ls())
# 결정트리 학습법
install.packages("tree")
library(tree)
rm(list=ls())
library(moments)
library(nortest)
library(car)
library(corrplot)
# set directory
setwd("C:/Users/wndy4/Desktop/Project_DEMA")
train = read.csv('train set.csv',header = T,stringsAsFactors = F)
valid = read.csv('validation set.csv',header = T,stringsAsFactors = F)
test = read.csv('test set.csv',header = T,stringsAsFactors = F)
train = train[,-c(1,2,3)]
valid = valid[,-c(1,2,3)]
test = test[,-c(1,2,3)]
Y = train$평균매매가
X1 = train$분양면적
X2 = train$단지세대수
X3 = train$개별세대수
X4 = train$노후도
X5 = train$역세권점수
X6 = train$평균전세가
X7 = train$토지면적
X8 = train$문화시설수
X9 = train$쇼핑시설수
X10 = train$스타벅스
X11 = train$평균인구수
X12 = train$평균세대수
X13 = train$교육시설수
X14 = train$개발호재
X15 = train$평균혼인건수
# 변수 설정
Y = train$평균매매가
X1 = train$분양면적
X2 = train$단지세대수
X3 = train$개별세대수
X4 = train$노후도
X5 = train$역세권점수
X6 = train$평균전세가
X7 = train$토지면적
X8 = train$문화시설수
X9 = train$쇼핑시설수
X10 = train$스타벅스
X11 = train$평균인구수
X12 = train$평균세대수
X13 = train$교육시설수
X14 = train$개발호재
X15 = train$평균혼인건수
# 회귀 적합
reg.train = lm(Y~X1+X2+X3+X4+X5+X6+X7+X8+X9+X10+X11+X12+X13+factor(X14)+X15,data=train)
coef = reg.train$coefficients
valid.mat = valid
valid.mat[,1] = 1
valid.mat = as.matrix(valid.mat)
# predict valid.hat
valid.hat = as.vector(coef%*%t(valid.mat))
e = valid$평균매매가 - valid.hat
n = nrow(valid)
p = 15
# MSE 구하기
sum(e^2)/(n-p-1)
SSE.valid = sum(e^2)
SST.valid = sum((valid$평균매매가-mean(valid$평균매매가))^2)
SSE.valid
SST.valid
ad.R2.valid = 1-((SSE.valid/(n-p-1))/(SST.valid/(n-1)))
ad.R2.valid # 0.7881567
## 1. 원래 모델에 variable selection
reg.train1 = lm(Y~1,data=train)
# stepwise selection
step(reg.train1, scope = list(upper = reg.train,lower = reg.train1),direction = 'both')
# Y ~ X6 + X4 + X13 + factor(X14) + X5 + X9 + X11 + X12 + X2 + X15 + X7 + x8
# 빠진 변수 : X1, X3, X10
reg.train.stepwise1 = lm(Y ~ X2+X4+X5+X6+X7+X8+X9+X11+X12+X13+factor(X14)+X15,data=train)
coef.stepwise1 = reg.train.stepwise1$coefficients
summary(reg.train.stepwise1)
summary(reg.train)
# predict valid.hat
valid.mat1 = valid[,-c(2,4,11)]
valid.mat1[,1] = 1
valid.mat1 = as.matrix(valid.mat1)
valid.hat1 = as.vector(coef.stepwise1%*%t(valid.mat1))
e1 = valid$평균매매가 - valid.hat1
n = nrow(valid)
p1 = 12
# MSE 구하기
sum(e1^2)/(n-p1-1)
# 기존 1220.949에서 1217.055로 감소
# adjusted R2 구하기
SSE.valid1 = sum(e1^2)
# 기존 1220.949에서 1217.055로 감소
# adjusted R2 구하기
SSE.valid1 = sum(e1^2)
SST.valid1 = sum((valid$평균매매가-mean(valid$평균매매가))^2)
SSE.valid1
SST.valid1
ad.R2.valid1 = 1-((SSE.valid1/(n-p1-1))/(SST.valid1/(n-1)))
ad.R2.valid1 # 0.7888323으로 기존보다 증가
sqrt(vif(reg.train.stepwise1))>sqrt(10)
cor(Y,X11)
# vif > 10 다중공선성이 존재한다고 볼 수 있다.
# X11과 X12이 다중공선성이 존재한다고 볼 수 있다.
# X11 : 평균인구수
# X12 : 평균세대수
# 평균인구수와 평균세대수의 상관관계 분석
cor(X11,X12)
cor(Y,X11)
cor(Y,X12)
## 2. X11 제거 후 variable selection 진행
reg.train2 = lm(Y~X1+X2+X3+X4+X5+X6+X7+X8+X9+X10+X12+X13+factor(X14)+X15,data=train)
summary(reg.train2)
# validation set 을 통한 검증
coef2 = reg.train2$coefficients
valid.mat2 = valid[,-12]
valid.mat2[,1] = 1
valid.mat2 = as.matrix(valid.mat2)
# predict valid.hat
valid.hat2 = as.vector(coef2%*%t(valid.mat2))
e2 = valid$평균매매가 - valid.hat2
n = nrow(valid)
p2 = 14
# MSE
sum(e2^2)/(n-p2-1)
# MSE 구하기
sum(e^2)/(n-p-1)
x11()
plot(reg.train2)
x11()
plot(resid(reg.train2))
x11()
plot(resid(reg.train2)~fitted(reg.train2))
durbinWatsonTest(reg.train2)
# variable selection 진행
reg.train1 = lm(Y~1,data=train)
# stepwise selection
step(reg.train1, scope = list(upper = reg.train2,lower = reg.train1),direction = 'both')
# Y ~ X6 + X4 + X13 + X10 + factor(X14) + X5 + X9 + X7 + x1 + X15 + X2 + X3
# 빠진 변수 : X8, X12
reg.train.stepwise2 = lm(Y ~ X2+X3+X4+X5+X6+X7+X9+X10+X13+factor(X14)+X15,data=train)
coef.stepwise2 = reg.train.stepwise1$coefficients
coef.stepwise2 = reg.train.stepwise2$coefficients
summary(reg.train.stepwise2)
# mse 비교
valid.mat2_2 = valid[,-c(9,12,13)]
valid.mat2_2[,1] = 1
valid.mat2_2 = as.matrix(valid.mat2_2)
# predict valid.hat
valid.hat2_2 = as.vector(coef.stepwise2%*%t(valid.mat2_2))
# Y ~ X6 + X4 + X13 + X10 + factor(X14) + X5 + X9 + X7 + x1 + X15 + X2 + X3
# 빠진 변수 : X8, X12
reg.train.stepwise2 = lm(Y ~ X1+X2+X3+X4+X5+X6+X7+X9+X10+X13+factor(X14)+X15,data=train)
coef.stepwise2 = reg.train.stepwise2$coefficients
# mse 비교
valid.mat2_2 = valid[,-c(9,12,13)]
valid.mat2_2[,1] = 1
valid.mat2_2 = as.matrix(valid.mat2_2)
# predict valid.hat
valid.hat2_2 = as.vector(coef.stepwise2%*%t(valid.mat2_2))
p2_2 = 12
# MSE
sum(e2_2^2)/(n-p2_2-1)
e2_2 = valid$평균매매가 - valid.hat2_2
n = nrow(valid)
p2_2 = 12
# MSE
sum(e2_2^2)/(n-p2_2-1)
## 3. X12 제거 후 variable selection 진행
reg.train3 = lm(Y~X1+X2+X3+X4+X5+X6+X7+X8+X9+X10+X11+X13+factor(X14)+X15,data=train)
summary(reg.train3)
# validation set 을 통한 검증
coef3 = reg.train3$coefficients
valid.mat3 = valid[,-13]
valid.mat3[,1] = 1
valid.mat3 = as.matrix(valid.mat3)
# predict valid.hat
valid.hat3 = as.vector(coef3%*%t(valid.mat3))
e3 = valid$평균매매가 - valid.hat3
n = nrow(valid)
p3 = 14
# MSE
sum(e3^2)/(n-p3-1)
x11()
plot(reg.train3)
x11()
plot(resid(reg.train3))
x11()
plot(resid(reg.train3)~fitted(reg.train3))
durbinWatsonTest(reg.train3)
# variable selection 진행
reg.train1 = lm(Y~1,data=train)
# stepwise selection
step(reg.train1, scope = list(upper = reg.train3,lower = reg.train1),direction = 'both')
# Y ~ X6 + X4 + X13 + X10 + factor(X14) + X5 + X9 + X11 + x7 + X15 + X1 + X2
# 빠진 변수 : X3, X8
reg.train.stepwise3 = lm(Y ~ X1+X2+X4+X5+X6+X7+X9+X10+X11+X13+factor(X14)+X15,data=train)
coef.stepwise3 = reg.train.stepwise3$coefficients
summary(reg.train.stepwise3)
# mse 비교
valid.mat3_1 = valid[,-c(4,9,13)]
valid.mat3_1[,1] = 1
valid.mat3_1 = as.matrix(valid.mat3_1)
# predict valid.hat
valid.hat3_1 = as.vector(coef.stepwise3%*%t(valid.mat3_1))
e3_1 = valid$평균매매가 - valid.hat3_1
n = nrow(valid)
p3_1 = 12
# MSE
sum(e3_1^2)/(n-p3_1-1)
